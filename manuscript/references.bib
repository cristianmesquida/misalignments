@article{aczel2018,
  title = {Quantifying {{Support}} for the {{Null Hypothesis}} in {{Psychology}}: {{An Empirical Investigation}}},
  shorttitle = {Quantifying {{Support}} for the {{Null Hypothesis}} in {{Psychology}}},
  author = {Aczel, Balazs and Palfi, Bence and Szollosi, Aba and Kovacs, Marton and Szaszi, Barnabas and Szecsi, Peter and Zrubka, Mark and Gronau, Quentin F. and {van den Bergh}, Don and Wagenmakers, Eric-Jan},
  year = 2018,
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {357--366},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918773742},
  urldate = {2023-04-28},
  abstract = {In the traditional statistical framework, nonsignificant results leave researchers in a state of suspended disbelief. In this study, we examined, empirically, the treatment and evidential impact of nonsignificant results. Our specific goals were twofold: to explore how psychologists interpret and communicate nonsignificant results and to assess how much these results constitute evidence in favor of the null hypothesis. First, we examined all nonsignificant findings mentioned in the abstracts of the 2015 volumes of Psychonomic Bulletin \& Review, Journal of Experimental Psychology: General, and Psychological Science (N = 137). In 72\% of these cases, nonsignificant results were misinterpreted, in that the authors inferred that the effect was absent. Second, a Bayes factor reanalysis revealed that fewer than 5\% of the nonsignificant findings provided strong evidence (i.e., BF01 {$>$} 10) in favor of the null hypothesis over the alternative hypothesis. We recommend that researchers expand their statistical tool kit in order to correctly interpret nonsignificant results and to be able to evaluate the evidence for and against the null hypothesis.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/2MSLLWGN/Aczel et al. - 2018 - Quantifying Support for the Null Hypothesis in Psy.pdf}
}

@article{anvari_sesoi,
  ids = {anvariUsingAnchorbasedMethods2021},
  title = {Using Anchor-Based Methods to Determine the Smallest Effect Size of Interest},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = 2021,
  journal = {Journal of Experimental Social Psychology},
  volume = {96},
  pages = {104159},
  doi = {10.1016/j.jesp.2021.104159},
  abstract = {Effect sizes are an important outcome of quantitative research, but few guidelines exist that explain how researchers can determine which effect sizes are meaningful. Psychologists often want to study effects that are large enough to make a difference to people's subjective experience. Thus, subjective experience is one way to gauge the meaningfulness of an effect. We propose and illustrate one method for how to quantify the smallest subjectively experienced difference---the smallest change in an outcome measure that individuals consider to be meaningful enough in their subjective experience such that they are willing to rate themselves as feeling different---using an anchor-based method with a global rating of change question applied to the positive and negative affect scale. We provide a step-by-step guide for the questions that researchers need to consider in deciding whether and how to use the anchor-based method, and we make explicit the assumptions of the method that future research can examine. For researchers interested in people's subjective experiences, this anchor-based method provides one way to specify a smallest effect size of interest, which allows researchers to interpret observed results in terms of their theoretical and practical significance.},
  langid = {english},
  keywords = {Minimum important difference,Negative affect,Positive affect,Practical significance,Smallest effect size of interest,Smallest subjectively experienced difference,Subjectively experienced difference},
  file = {/Users/cristian/Zotero/storage/NFQMX69B/Anvari and Lakens - 2021 - Using anchor-based methods to determine the smalle.pdf}
}

@article{bachelez2015,
  title = {Tofacitinib versus Etanercept or Placebo in Moderate-to-Severe Chronic Plaque Psoriasis: A Phase 3 Randomised Non-Inferiority Trial},
  shorttitle = {Tofacitinib versus Etanercept or Placebo in Moderate-to-Severe Chronic Plaque Psoriasis},
  author = {Bachelez, Herv{\'e} and {van de Kerkhof}, Peter C. M. and Strohal, Robert and Kubanov, Alexey and Valenzuela, Fernando and Lee, Joo-Heung and Yakusevich, Vladimir and Chimenti, Sergio and Papacharalambous, Jocelyne and Proulx, James and Gupta, Pankaj and Tan, Huaming and Tawadrous, Margaret and Valdez, Hernan and Wolk, Robert and {OPT Compare Investigators}},
  year = 2015,
  month = aug,
  journal = {Lancet (London, England)},
  volume = {386},
  number = {9993},
  pages = {552--561},
  issn = {1474-547X},
  doi = {10.1016/S0140-6736(14)62113-9},
  abstract = {BACKGROUND: New therapeutic options are needed for patients with psoriasis. Tofacitinib, an oral Janus kinase inhibitor, is being investigated as a treatment for moderate-to-severe chronic plaque psoriasis. In this study, we aimed to compare two tofacitinib doses with high-dose etanercept or placebo in this patient population. METHODS: In this phase 3, randomised, multicentre, double-dummy, placebo-controlled, 12-week, non-inferiority trial, adult patients with chronic stable plaque psoriasis (for {$\geq$}12 months) who were candidates for systemic or phototherapy and had a Psoriasis Area and Severity Index (PASI) score of 12 or higher and a Physician's Global Assessment (PGA) of moderate or severe, and had failed to respond to, had a contraindication to, or were intolerant to at least one conventional systemic therapy, were enrolled from 122 investigational dermatology centres worldwide. Eligible patients were randomly assigned in a 3:3:3:1 ratio to receive tofacitinib 5 mg or 10 mg twice daily at about 12 h intervals, etanercept 50 mg subcutaneously twice weekly at about 3-4 day intervals, or placebo. Randomisation was done by a computer-generated randomisation schedule, and all patients and study personnel were masked to treatment assignment. The co-primary endpoints were the proportion of patients at week 12 with at least a 75\% reduction in the PASI score from baseline (PASI75 response) and the proportion of patients achieving a PGA score of "clear" or "almost clear" (PGA response), analysed in the full analysis set (all patients who were randomised and received at least one dose of study drug). This study is registered with ClinicalTrials.gov, number NCT01241591. FINDINGS: Between Nov 29, 2010, and Sept 13, 2012, we enrolled 1106 eligible adult patients with chronic plaque psoriasis and randomly assigned them to the four treatment groups (330 to tofacitinib 5 mg twice daily, 332 to tofacitinib 10 mg twice daily, 336 to etanercept 50 mg twice weekly, and 108 to placebo). Of these patients, 1101 actually received their assigned study medication (329 in the tofactinib 5 mg group, 330 in the tofacitinib 10 mg group, 335 in the etanercept group, and 107 in the placebo group). At week 12, PASI75 responses were recorded in 130 (39{$\cdot$}5\%) of 329 patients in the tofacitinib 5 mg group, 210 (63{$\cdot$}6\%) of 330 in the tofacitinib 10 mg group, 197 (58{$\cdot$}8\%) of 335 in the etanercept group, and six (5{$\cdot$}6\%) of 107 in the placebo group. A PGA response was achieved by 155 (47{$\cdot$}1\%) of 329 patients in the tofacitinib 5 mg group, 225 (68{$\cdot$}2\%) of 330 in the tofacitinib 10 mg group, 222 (66{$\cdot$}3\%) of 335 in the etanercept group, and 16 (15{$\cdot$}0\%) of 107 in the placebo group. The rate of adverse events was similar across the four groups, with serious adverse events occurring in seven (2\%) of 329 patients in the tofacitinib 5 mg group, five (2\%) of 330 in the tofacitinib 10 mg group, seven (2\%) of 335 in the etanercept group, and two (2\%) of 107 in the placebo group. Three (1\%) of 329 patients in the tofacitinib 5 mg group, ten (3\%) of 330 in the tofacitinib 10 mg group, 11 (3\%) of 335 in the etanercept group, and four (4\%) of 107 patients in the placebo group discontinued their assigned treatment because of adverse events. INTERPRETATION: In patients with moderate-to-severe plaque psoriasis, the 10 mg twice daily dose of tofacitinib was non-inferior to etanercept 50 mg twice weekly and was superior to placebo, but the 5 mg twice daily dose did not show non-inferiority to etanercept 50 mg twice weekly. The adverse event rates over 12 weeks were similar for tofacitinib and etanercept. This study indicates that in the future tofacitinib could provide a convenient and well-tolerated therapeutic option for patients with moderate-to-severe plaque psoriasis. FUNDING: Pfizer Inc.},
  langid = {english},
  pmid = {26051365},
  keywords = {{Anti-Inflammatory Agents, Non-Steroidal},{Dose-Response Relationship, Drug},{Receptors, Tumor Necrosis Factor},\{Anti-Inflammatory Agents\vphantom\},\{Dose-Response Relationship\vphantom\},\{Receptors\vphantom\},Adult,Chronic Disease,Double-Blind Method,Drug Administration Schedule,Drug\vphantom\{\},Etanercept,Female,Humans,Immunoglobulin G,Male,Middle Aged,Non-Steroidal\vphantom\{\},Piperidines,Protein Kinase Inhibitors,Psoriasis,Pyrimidines,Pyrroles,Treatment Outcome,Tumor Necrosis Factor\vphantom\{\}}
}

@book{baguley2018,
  title = {Serious {{Stat}}: {{A}} Guide to Advanced Statistics for the Behavioral Sciences},
  shorttitle = {Serious {{Stat}}},
  author = {Baguley, Thomas},
  year = 2018,
  month = jan,
  publisher = {Bloomsbury Publishing},
  abstract = {Ideal for experienced students and researchers in the social sciences who wish to refresh or extend their understanding of statistics, and to apply advanced statistical procedures using SPSS or R. Key theory is reviewed and illustrated with examples of how to apply these concepts using real data.},
  googlebooks = {0ZhGEAAAQBAJ},
  isbn = {978-0-230-36355-7},
  langid = {english},
  keywords = {Psychology / Research \& Methodology}
}

@article{bland_2011,
  title = {Comparisons against Baseline within Randomised Groups Are Often Used and Can Be Highly Misleading},
  author = {Bland, J Martin and Altman, Douglas G},
  year = 2011,
  month = dec,
  journal = {Trials},
  volume = {12},
  pages = {264},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-12-264},
  urldate = {2023-06-30},
  abstract = {Background In randomised trials, rather than comparing randomised groups directly some researchers carry out a significance test comparing a baseline with a final measurement separately in each group. Methods We give several examples where this has been done. We use simulation to demonstrate that the procedure is invalid and also show this algebraically. Results This approach is biased and invalid, producing conclusions which are, potentially, highly misleading. The actual alpha level of this procedure can be as high as 0.50 for two groups and 0.75 for three. Conclusions Randomised groups should be compared directly by two-sample methods and separate tests against baseline are highly misleading.},
  pmcid = {PMC3286439},
  pmid = {22192231},
  file = {/Users/cristian/Zotero/storage/RS8JECRF/Bland and Altman - 2011 - Comparisons against baseline within randomised gro.pdf}
}

@article{bland2015,
  title = {Best (but Oft Forgotten) Practices: Testing for Treatment Effects in Randomized Trials by Separate Analyses of Changes from Baseline in Each Group Is a Misleading Approach},
  shorttitle = {Best (but Oft Forgotten) Practices},
  author = {Bland, J. Martin and Altman, Douglas G.},
  year = 2015,
  month = nov,
  journal = {The American Journal of Clinical Nutrition},
  volume = {102},
  number = {5},
  pages = {991--994},
  issn = {1938-3207},
  doi = {10.3945/ajcn.115.119768},
  abstract = {Researchers often analyze randomized trials and other comparative studies by separate analysis of changes from baseline in each parallel group. This may be the only analysis presented or it may be in addition to the direct comparison of allocated groups. We illustrate this by reference to 3 recently published nutritional trials. We show why this method of analysis may be highly misleading and may produce type I errors far greater than the 5\% that we expect. We recommend direct comparison of means between groups with the use of baseline as a covariate if required.},
  langid = {english},
  pmid = {26354536},
  keywords = {Guidelines as Topic,Humans,Nutritional Sciences,Randomized Controlled Trials as Topic,Statistics as Topic,United Kingdom}
}

@article{buttner2020,
  ids = {buttnerAreQuestionableResearch2020},
  title = {Are Questionable Research Practices Facilitating New Discoveries in Sport and Exercise Medicine? {{The}} Proportion of Supported Hypotheses Is Implausibly High},
  author = {B{\"u}ttner, Fionn and Toomey, Elaine and McClean, Shane and Roe, Mark and Delahunt, Eamonn},
  year = 2020,
  journal = {British journal of sports medicine},
  volume = {54},
  number = {22},
  pages = {1365--1371},
  publisher = {{BMJ Publishing Group Ltd and British Association of Sport and Exercise Medicine}},
  chapter = {Education review},
  isbn = {0306-3674},
  pmid = {32699001},
  keywords = {education,methodological,research,sport,statistics}
}

@article{cho_abe2013,
  title = {Is Two-Tailed Testing for Directional Research Hypotheses Tests Legitimate?},
  author = {Cho, Hyun-Chul and Abe, Shuzo},
  year = 2013,
  journal = {Journal of Business Research},
  volume = {66},
  number = {9},
  pages = {1261--1266},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2012.02.023},
  abstract = {This paper demonstrates that there is currently a widespread misuse of two-tailed testing for directional research hypotheses tests. One probable reason for this overuse of two-tailed testing is the seemingly valid beliefs that two-tailed testing is more conservative and safer than one-tailed testing. However, the authors examine the legitimacy of this notion and find it to be flawed. A second and more fundamental cause of the current problem is the pervasive oversight in making a clear distinction between the research hypothesis and the statistical hypothesis. Based upon the explicated, sound relationship between the research and statistical hypotheses, the authors propose a new scheme of hypothesis classification to facilitate and clarify the proper use of statistical hypothesis testing in empirical research.},
  keywords = {Hypothesis testing,One-tailed testing,Research hypothesis in existential form,Research hypothesis in non-existential form,Statistical hypothesis,Two-tailed testing}
}

@article{cook_multiplicity_1996,
  title = {Multiplicity {{Considerations}} in the {{Design}} and {{Analysis}} of {{Clinical Trials}}},
  author = {Cook, Richard J. and Farewell, Vern T.},
  year = 1996,
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {159},
  number = {1},
  eprint = {2983471},
  eprinttype = {jstor},
  pages = {93--110},
  publisher = {[Wiley, Royal Statistical Society]},
  issn = {0964-1998},
  doi = {10.2307/2983471},
  urldate = {2024-05-03},
  abstract = {The need for efficient use of available resources in medical research has led to the increased appeal of clinical trial designs based on multiple responses, multiple treatment arms and repeated tests of significance. In recent years there has been considerable methodological work pertaining to these types of multiple comparison, with the common objective typically being the control of the experimental type I error rate. Here we reconsider the appropriateness of these objectives in a variety of contexts and suggest that multiple-comparison procedures are frequently adopted unnecessarily. In particular we argue that, provided that a select number of important well-defined clinical questions are specified at the design, there are situations in which multiple tests of significance can be performed without control of the experimental type I error rate. The primary restriction for this to be reasonable is that test results are interpreted marginally.},
  file = {/Users/cristian/Zotero/storage/YHAJAJ84/Cook and Farewell - 1996 - Multiplicity Considerations in the Design and Anal.pdf}
}

@article{dimitrov2003,
  title = {Pretest-Posttest Designs and Measurement of Change},
  author = {Dimitrov, Dimiter M. and Rumrill, Phillip D.},
  year = 2003,
  journal = {Work},
  volume = {20},
  number = {2},
  pages = {159--165},
  issn = {1051-9815},
  abstract = {The article examines issues involved in comparing groups and measuring change with pretest and posttest data. Different pretest-posttest designs are presented in a manner that can help rehabilitation professionals to better understand and determine effects resulting from selected interventions. The reliability of gain scores in pretest-posttest measurement is also discussed in the context of rehabilitation research and practice.},
  langid = {english},
  pmid = {12671209},
  keywords = {{Data Interpretation, Statistical},Analysis of Variance,Humans,Rehabilitation,Research Design,Treatment Outcome,United States}
}

@article{ditroilo_exploratory,
  ids = {ditroiloExploratoryResearchSport2025},
  title = {Exploratory Research in Sport and Exercise Science: {{Perceptions}}, Challenges, and Recommendations},
  shorttitle = {Exploratory Research in Sport and Exercise Science},
  author = {Ditroilo, Massimiliano and Mesquida, Cristian and Abt, Grant and Lakens, Dani{\"e}l},
  year = 2025,
  month = apr,
  journal = {Journal of Sports Sciences},
  pages = {1--13},
  issn = {1466-447X},
  doi = {10.1080/02640414.2025.2486871},
  abstract = {Quantitative exploratory research implies a flexible examination of a dataset with the purpose of finding patterns, associations, and interactions between variables to help formulate a hypothesis, which should be severely tested in a future confirmatory study. In many fields, including sport and exercise science, exploratory research is not openly reported, a practice that leads to serious problems. At the same time, exploration is a crucial step in scientific knowledge generation, and a substantial proportion of studies will be exploratory in nature, or include both confirmatory and exploratory analyses. Using a flowchart, we review how data are typically collected and used, and we distinguish exploratory from confirmatory studies by arguing that data-driven analyses, where the Type I and Type II error cannot be controlled, is what characterises exploratory research. We ask which factors increase the quality and value of exploratory analyses, and highlight large sample sizes, uncommon sample compositions, rigorous data collection, widely used measures, observing a logical and coherent pattern across multiple variables, and the potential for generating new research questions as the main factors. Finally, we provide guidelines for carrying out and transparently writing up an exploratory study.},
  langid = {english},
  pmid = {40197233},
  keywords = {confirmatory,data analysis,error control,Hypothesis testing,questionable research practices,theory-driven analysis},
  file = {/Users/cristian/Zotero/storage/2EHKHQJC/Ditroilo et al. - 2025 - Exploratory research in sport and exercise science.pdf}
}

@article{dmitrienko2013,
  title = {Traditional Multiplicity Adjustment Methods in Clinical Trials},
  author = {Dmitrienko, Alex and D'Agostino Sr, Ralph},
  year = 2013,
  journal = {Statistics in Medicine},
  volume = {32},
  number = {29},
  pages = {5172--5218},
  issn = {1097-0258},
  doi = {10.1002/sim.5990},
  urldate = {2023-07-06},
  abstract = {This tutorial discusses important statistical problems arising in clinical trials with multiple clinical objectives based on different clinical variables, evaluation of several doses or regiments of a new treatment, analysis of multiple patient subgroups, etc. Simultaneous assessment of several objectives in a single trial gives rise to multiplicity. If unaddressed, problems of multiplicity can undermine integrity of statistical inferences. The tutorial reviews key concepts in multiple hypothesis testing and introduces main classes of methods for addressing multiplicity in a clinical trial setting. General guidelines for the development of relevant and efficient multiple testing procedures are presented on the basis of application-specific clinical and statistical information. Case studies with common multiplicity problems are used to motivate and illustrate the statistical methods presented in the tutorial, and software implementation of the multiplicity adjustment methods is discussed. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {clinical trials,multiple testing procedures,multiplicity adjustments,multiplicity problems,type I error rate},
  file = {/Users/cristian/Zotero/storage/RV4CJZ7L/Dmitrienko and D'Agostino Sr - 2013 - Traditional multiplicity adjustment methods in cli.pdf;/Users/cristian/Zotero/storage/V96WMWUV/sim.html}
}

@techreport{ema2017,
  title = {Guideline on Multiplicity Issues in Clinical Trials},
  author = {{European Medical Agency}},
  year = 2017
}

@techreport{fda2022,
  title = {Multiple {{Endpoints}} in {{Clinical Trials}} - {{Guidance}} for {{Industry}}},
  author = {{Food Drug Administration}},
  year = 2022,
  langid = {english},
  file = {/Users/cristian/Zotero/storage/2CWHXZMX/Multiple Endpoints in Clinical Trials - Guidance f.pdf}
}

@article{frick1996,
  title = {The Appropriate Use of Null Hypothesis Testing},
  author = {Frick, Robert W.},
  year = 1996,
  journal = {Psychological Methods},
  volume = {1},
  number = {4},
  pages = {379--390},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.1.4.379},
  abstract = {The many criticisms of null hypothesis testing suggest when it is not useful and what it should not be used for. This article explores when and why its use is appropriate. Null hypothesis testing is insufficient when size of effect is important, but it is ideal for testing ordinal claims relating the order of conditions, which are common in psychology. Null hypothesis testing also is insufficient for determining beliefs, but it is ideal for demonstrating sufficient evidential strength to support an ordinal claim, with sufficient evidence being 1 criterion for a finding entering the corpus of legitimate findings in psychology. The line between sufficient and insufficient evidence is currently set at p {$<$}.05; there is little reason for allowing experimenters to select their own value of alpha. Thus null hypothesis testing is an optimal method for demonstrating sufficient evidence for an ordinal claim. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing,Statistical Analysis,Statistical Validity},
  file = {/Users/cristian/Zotero/storage/A7TYG422/1996-06601-005.html}
}

@article{garcia-sifuentes2021,
  title = {Reporting and Misreporting of Sex Differences in the Biological Sciences},
  author = {{Garcia-Sifuentes}, Yesenia and Maney, Donna L},
  year = 2021,
  journal = {eLife},
  volume = {10},
  pages = {e70817},
  issn = {2050-084X},
  doi = {10.7554/eLife.70817},
  urldate = {2024-01-25},
  abstract = {As part of an initiative to improve rigor and reproducibility in biomedical research, the U.S. National Institutes of Health now requires the consideration of sex as a biological variable in preclinical studies. This new policy has been interpreted by some as a call to compare males and females with each other. Researchers testing for sex differences may not be trained to do so, however, increasing risk for misinterpretation of results. Using a list of recently published articles curated by Woitowich et al. (eLife, 2020; 9:e56344), we examined reports of sex differences and non-differences across nine biological disciplines. Sex differences were claimed in the majority of the 147 articles we analyzed; however, statistical evidence supporting those differences was often missing. For example, when a sex-specific effect of a manipulation was claimed, authors usually had not tested statistically whether females and males responded differently. Thus, sex-specific effects may be over-reported. In contrast, we also encountered practices that could mask sex differences, such as pooling the sexes without first testing for a difference. Our findings support the need for continuing efforts to train researchers how to test for and report sex differences in order to promote rigor and reproducibility in biomedical research., Biomedical research has a long history of including only men or male laboratory animals in studies. To address this disparity, the United States National Institutes of Health (NIH) rolled out a policy in 2016 called Sex as a Biological Variable (or SABV). The policy requires researchers funded by the NIH to include males and females in every experiment unless there is a strong justification not to, such as studies of ovarian cancer. Since then, the number of research papers including both sexes has continued to grow., Although the NIH does not require investigators to compare males and females, many researchers have interpreted the SABV policy as a call to do so. This has led to reports of sex differences that would otherwise have been unrecognized or ignored. However, researchers may not be trained on how best to test for sex differences in their data, and if the data are not analyzed appropriately this may lead to misleading interpretations., Here, Garcia-Sifuentes and Maney have examined the methods of 147 papers published in 2019 that included both males and females. They discovered that more than half of these studies had reported sex differences, but these claims were not always backed by statistical evidence. Indeed, in a large majority (more than 70\%) of the papers describing differences in how males and females responded to a treatment, the impact of the treatment was not actually statistically compared between the sexes. This suggests that sex-specific effects may be over-reported. In contrast, Garcia-Sifuentes and Maney also encountered instances where an effect may have been masked due to data from males and females being pooled together without testing for a difference first., These findings reveal how easy it is to draw misleading conclusions from sex-based data. Garcia-Sifuentes and Maney hope their work raises awareness of this issue and encourages the development of more training materials for researchers.},
  pmcid = {PMC8562995},
  pmid = {34726154},
  file = {/Users/cristian/Zotero/storage/84MU3MU9/Garcia-Sifuentes and Maney - Reporting and misreporting of sex differences in t.pdf}
}

@article{gigerenzer2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  year = 2004,
  month = nov,
  journal = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.033},
  urldate = {2022-03-29},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the ``null ritual'' consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english},
  keywords = {Collective illusions,Editors,Rituals,Statistical significance,Textbooks},
  file = {/Users/cristian/Zotero/storage/XR9TDHZG/S1053535704000927.html}
}

@article{hand1994,
  title = {Deconstructing {{Statistical Questions}}},
  author = {Hand, David J.},
  year = 1994,
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {157},
  number = {3},
  eprint = {2983526},
  eprinttype = {jstor},
  pages = {317--356},
  publisher = {[Wiley, Royal Statistical Society]},
  issn = {0964-1998},
  doi = {10.2307/2983526},
  urldate = {2024-01-03},
  abstract = {Too much current statistical work takes a superficial view of the client's research question, adopting techniques which have a solid history, a sound mathematical basis or readily available software, but without considering in depth whether the questions being answered are in fact those which should be asked. Examples, some familiar and others less so, are given to illustrate this assertion. It is clear that establishing the mapping from the client's domain to a statistical question is one of the most difficult parts of a statistical analysis. It is a part in which the responsibility is shared by both client and statistician. A plea is made for more research effort to go in this direction and some suggestions are made for ways to tackle the problem.},
  file = {/Users/cristian/Zotero/storage/RLGAB6U6/Hand - 1994 - Deconstructing Statistical Questions.pdf}
}

@article{hempel1945,
  title = {Studies in the {{Logic}} of {{Confirmation}}},
  author = {Hempel, Carl G.},
  year = 1945,
  journal = {Mind},
  volume = {54},
  number = {213},
  eprint = {2250886},
  eprinttype = {jstor},
  pages = {1--26},
  publisher = {[Oxford University Press, Mind Association]},
  issn = {0026-4423},
  urldate = {2024-01-19},
  file = {/Users/cristian/Zotero/storage/NH6NGS4I/Hempel - 1945 - Studies in the Logic of Confirmation (I.).pdf}
}

@book{hempel1966,
  title = {Philosophy of Natural {{Science}}},
  author = {Hempel, Carl G.},
  year = 1966,
  series = {Philosophy of Natural {{Science}}},
  pages = {ix, 116},
  publisher = {Prentice-Hall},
  address = {Oxford, England},
  file = {/Users/cristian/Zotero/storage/JLCHGT39/1966-11595-000.html}
}

@article{huck1975,
  title = {Using a Repeated Measures {{ANOVA}} to Analyze the Data from a Pretest-Posttest Design: {{A}} Potentially Confusing Task},
  shorttitle = {Using a Repeated Measures {{ANOVA}} to Analyze the Data from a Pretest-Posttest Design},
  author = {Huck, Schuyler W. and McLean, Robert A.},
  year = 1975,
  journal = {Psychological Bulletin},
  volume = {82},
  number = {4},
  pages = {511--518},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/h0076767},
  abstract = {The pretest-posttest control group design (or an extension of it) is a highly prestigious experimental design. A popular analytic strategy involves subjecting the data provided by this design to a repeated measures analysis of variance (ANOVA). Unfortunately, the statistical results yielded by this type of analysis can easily be misinterpreted, since the score model underlying the analysis is not correct. Examples from recently published articles are used to demonstrate that this statistical procedure has led to (a) incorrect statements regarding treatment effects, (b) completely redundant reanalyses of the same data, and (c) problems with respect to post hoc investigations. 2 alternative strategies-gain scores and covariance-are discussed and compared. (19 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Analysis of Variance,Experimental Design},
  file = {/Users/cristian/Zotero/storage/KNCYGZUM/1975-26619-001.html}
}

@article{kahan_estimands2024,
  title = {The Estimands Framework: A Primer on the {{ICH E9}}({{R1}}) Addendum},
  shorttitle = {The Estimands Framework},
  author = {Kahan, Brennan C. and Hindley, Joanna and Edwards, Mark and Cro, Suzie and Morris, Tim P.},
  year = 2024,
  month = jan,
  journal = {BMJ},
  volume = {384},
  pages = {e076316},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2023-076316},
  urldate = {2025-03-20},
  abstract = {{$<$}p{$>$}Estimands can be used in studies of healthcare interventions to clarify the interpretation of treatment effects. The addendum to the ICH E9 harmonised guideline on statistical principles for clinical trials (ICH E9(R1)) describes a framework for using estimands as part of a study. This paper provides an overview of the estimands framework, as outlined in the addendum, with the aim of explaining why estimands are beneficial; clarifying the terminology being used; and providing practical guidance on using estimands to decide the appropriate study design, data collection, and estimation methods. This article illustrates how to use the estimands framework by applying it to an ongoing trial in emergency bowel surgery. Estimands can be a useful way of clarifying the exact research question being evaluated in a study, both to avoid misinterpretation and to ensure that study methods are aligned to the overall study objectives.{$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {38262663}
}

@article{kaiser1960,
  title = {Directional Statistical Decisions},
  author = {Kaiser, Henry F.},
  year = 1960,
  journal = {Psychological Review},
  volume = {67},
  number = {3},
  pages = {160--167},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/h0047595},
  abstract = {Concerning the traditional nondirectional 2-sided test of significance, the author argues that "we cannot logically make a directional statistical decision or statement when the null hypothesis is rejected on the basis of the direction of the difference in the observed means." Thus, this test "should almost never be used." He proposes that "almost without exception the directional two-sided test should replace" it (18 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing,Statistical Significance},
  file = {/Users/cristian/Zotero/storage/9N5JGY3H/1961-01476-001.html}
}

@article{lakens_2024,
  title = {The Benefits of Preregistration and {{Registered Reports}}},
  author = {Lakens, Dani{\"e}l and Mesquida, Cristian and Rasti, Sajedeh and Ditroilo, Massimiliano},
  year = 2024,
  month = dec,
  journal = {Evidence-Based Toxicology},
  volume = {2},
  number = {1},
  pages = {2376046},
  issn = {2833-373X},
  doi = {10.1080/2833373X.2024.2376046},
  urldate = {2025-02-17},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/IQDT6CXN/Lakens et al. - 2024 - The benefits of preregistration and Registered Rep.pdf}
}

@article{lakens_equivalence_2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta-Analyses}}},
  author = {Lakens, Dani{\"e}l},
  year = 2017,
  month = may,
  journal = {Social Psychological and Personality Science},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  langid = {english},
  keywords = {equivalence testing,null hypothesis significance testing,power analysis,research methods},
  file = {/Users/cristian/Zotero/storage/PZC95BMI/Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf}
}

@article{lawrance2020,
  ids = {lawranceWhatEstimandHow2020a},
  title = {What Is an Estimand \& How Does It Relate to Quantifying the Effect of Treatment on Patient-Reported Quality of Life Outcomes in Clinical Trials?},
  author = {Lawrance, Rachael and Degtyarev, Evgeny and Griffiths, Philip and Trask, Peter and Lau, Helen and D'Alessio, Denise and Griebsch, Ingolf and Wallenstein, Gudrun and Cocks, Kim and Rufibach, Kaspar},
  year = 2020,
  month = aug,
  journal = {Journal of Patient-Reported Outcomes},
  volume = {4},
  number = {1},
  pages = {68},
  issn = {2509-8020},
  doi = {10.1186/s41687-020-00218-5},
  urldate = {2024-08-29},
  abstract = {Published in 2019, a new addendum to the ICH E9 guideline presents the estimand framework as a systematic approach to ensure alignment among clinical trial objectives, trial execution/conduct, statistical analyses, and interpretation of results. The use of the estimand framework for describing clinical trial objectives has yet to be extensively considered in the context of patient-reported outcomes (PROs). We discuss the application of the estimand framework to PRO objectives when designing clinical trials in the future, with a focus on PRO outcomes in oncology trial settings as our example.},
  keywords = {Clinical trial,Design,Estimand,HRQoL,ICH,Objective,Patient,PRO,Treatment effect},
  file = {/Users/cristian/Zotero/storage/QUILXSF7/Lawrance et al. - 2020 - What is an estimand & how does it relate to quanti.pdf;/Users/cristian/Zotero/storage/QFLPMCKD/s41687-020-00218-5.html}
}

@article{li2016,
  title = {An Introduction to Multiplicity Issues in Clinical Trials: The What, Why, When and How},
  shorttitle = {An Introduction to Multiplicity Issues in Clinical Trials},
  author = {Li, Guowei and Taljaard, Monica and Van Den Heuvel, Edwin R. and Levine, Mitchell Ah. and Cook, Deborah J. and Wells, George A. and Devereaux, Philip J. and Thabane, Lehana},
  year = 2016,
  month = dec,
  journal = {International Journal of Epidemiology},
  pages = {dyw320},
  issn = {0300-5771, 1464-3685},
  doi = {10.1093/ije/dyw320},
  urldate = {2025-04-15},
  abstract = {In clinical trials it is not uncommon to face a multiple testing problem which can have an impact on both type I and type II error rates, leading to inappropriate interpretation of trial results. Multiplicity issues may need to be considered at the design, analysis and interpretation stages of a trial. The proportion of trial reports not adequately correcting for multiple testing remains substantial. The purpose of this article is to provide an introduction to multiple testing issues in clinical trials, and to reduce confusion around the need for multiplicity adjustments. We use a tutorial, question-and-answer approach to address the key issues of why, when and how to consider multiplicity adjustments in trials. We summarize the relevant circumstances under which multiplicity adjustments ought to be considered, as well as options for carrying out multiplicity adjustments in terms of trial design factors including Population, Intervention/Comparison, Outcome, Time frame and Analysis (PICOTA). Results are presented in an easy-to-use table and flow diagrams. Confusion about multiplicity issues can be reduced or avoided by considering the potential impact of multiplicity on type I and II errors and, if necessary pre-specifying statistical approaches to either avoid or adjust for multiplicity in the trial protocol or analysis plan.},
  langid = {english}
}

@article{lundberg2021,
  ids = {lundbergWhatYourEstimand2021a},
  title = {What {{Is Your Estimand}}? {{Defining}} the {{Target Quantity Connects Statistical Evidence}} to {{Theory}}},
  shorttitle = {What {{Is Your Estimand}}?},
  author = {Lundberg, Ian and Johnson, Rebecca and Stewart, Brandon M.},
  year = 2021,
  month = jun,
  journal = {American Sociological Review},
  volume = {86},
  number = {3},
  pages = {532--565},
  publisher = {SAGE Publications Inc},
  issn = {0003-1224},
  doi = {10.1177/00031224211004187},
  urldate = {2023-07-06},
  abstract = {We make only one point in this article. Every quantitative study must be able to answer the question: what is your estimand? The estimand is the target quantity---the purpose of the statistical analysis. Much attention is already placed on how to do estimation; a similar degree of care should be given to defining the thing we are estimating. We advocate that authors state the central quantity of each analysis---the theoretical estimand---in precise terms that exist outside of any statistical model. In our framework, researchers do three things: (1) set a theoretical estimand, clearly connecting this quantity to theory; (2) link to an empirical estimand, which is informative about the theoretical estimand under some identification assumptions; and (3) learn from data. Adding precise estimands to research practice expands the space of theoretical questions, clarifies how evidence can speak to those questions, and unlocks new tools for estimation. By grounding all three steps in a precise statement of the target quantity, our framework connects statistical evidence to theory.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/E5RG9FUW/Lundberg et al. - 2021 - What Is Your Estimand Defining the Target Quantit.pdf}
}

@article{machine_readable2021,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigor}} by {{Making Hypothesis Tests Machine-Readable}}},
  author = {Lakens, Dani{\"e}l and DeBruine, Lisa M.},
  year = 2021,
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {2515245920970949},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920970949},
  urldate = {2022-09-08},
  abstract = {Making scientific information machine-readable greatly facilitates its reuse. Many scientific articles have the goal to test a hypothesis, so making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine-readable. We believe there are two benefits to specifying a hypothesis test in such a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis tests become more transparent, falsifiable, and rigorous. Second, scientists benefit if information related to hypothesis tests in scientific articles is easily findable and reusable, for example, to perform meta-analyses, conduct peer review, and examine metascientific research questions. We examine what a machine-readable hypothesis test should look like and demonstrate the feasibility of machine-readable hypothesis tests in a real-life example using the fully operational prototype R package scienceverse.},
  langid = {english},
  keywords = {hypothesis testing,machine readability,metadata,scholarly communication}
}

@article{maxwell_2004,
  title = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}: {{Causes}}, {{Consequences}}, and {{Remedies}}},
  author = {Maxwell, Scott E.},
  year = 2004,
  journal = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {147--163},
  doi = {10.1037/1082-989X.9.2.147},
  abstract = {Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The "curse of multiplicities" plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Consequence,Effect Size (Statistical),Hypothesis Testing,Methodology,Psychology,Statistical Power,Type I Errors},
  file = {/Users/cristian/Zotero/storage/ZIE2UQD5/Maxwell - 2004 - The Persistence of Underpowered Studies in Psychol.pdf;/Users/cristian/Zotero/storage/P35K8BIW/2004-14114-001.html}
}

@article{mayo2006,
  title = {Severe {{Testing}} as a {{Basic Concept}} in a {{Neyman-Pearson Philosophy}} of {{Induction}}},
  author = {Mayo, Deborah G. and Spanos, Aris},
  year = 2006,
  journal = {The British Journal for the Philosophy of Science},
  volume = {57},
  number = {2},
  eprint = {3873470},
  eprinttype = {jstor},
  pages = {323--357},
  publisher = {[Oxford University Press, The British Society for the Philosophy of Science]},
  issn = {0007-0882},
  urldate = {2022-03-30},
  abstract = {Despite the widespread use of key concepts of the Neyman-Pearson (N-P) statistical paradigm-type I and II errors, significance levels, power, confidence levels-they have been the subject of philosophical controversy and debate for over 60 years. Both current and long-standing problems of N-P tests stem from unclarity and confusion, even among N-P adherents, as to how a test's (pre-data) error probabilities are to be used for (post-data) inductive inference as opposed to inductive behavior. We argue that the relevance of error probabilities is to ensure that only statistical hypotheses that have passed severe or probative tests are inferred from the data. The severity criterion supplies a meta-statistical principle for evaluating proposed statistical inferences, avoiding classic fallacies from tests that are overly sensitive, as well as those not sensitive enough to particular errors and discrepancies.}
}

@article{mazzolari_2022,
  title = {Myths and Methodologies: {{The}} Use of Equivalence and Non-Inferiority Tests for Interventional Studies in Exercise Physiology and Sport Science},
  shorttitle = {Myths and Methodologies},
  author = {Mazzolari, Raffaele and Porcelli, Simone and Bishop, David J. and Lakens, Dani{\"e}l},
  year = 2022,
  month = mar,
  journal = {Experimental Physiology},
  volume = {107},
  number = {3},
  pages = {201--212},
  issn = {1469-445X},
  doi = {10.1113/EP090171},
  abstract = {Exercise physiology and sport science have traditionally made use of the null hypothesis of no difference to make decisions about experimental interventions. In this article, we aim to review current statistical approaches typically used by exercise physiologists and sport scientists for the design and analysis of experimental interventions and to highlight the importance of including equivalence and non-inferiority studies, which address different research questions from deciding whether an effect is present. Initially, we briefly describe the most common approaches, along with their rationale, to investigate the effects of different interventions. We then discuss the main steps involved in the design and analysis of equivalence and non-inferiority studies, commonly performed in other research fields, with worked examples from exercise physiology and sport science scenarios. Finally, we provide recommendations to exercise physiologists and sport scientists who would like to apply the different approaches in future research. We hope this work will promote the correct use of equivalence and non-inferiority designs in exercise physiology and sport science whenever the research context, conditions, applications, researchers' interests or reasonable beliefs justify these approaches.},
  langid = {english},
  pmid = {35041233},
  keywords = {Exercise,Humans,intervention efficacy,methodology,Research Design,Sports,statistical review}
}

@article{mesquida_2023,
  title = {Publication Bias, Statistical Power and Reporting Practices in the {{Journal}} of {{Sports Sciences}}: Potential Barriers to Replicability},
  shorttitle = {Publication Bias, Statistical Power and Reporting Practices in the {{Journal}} of {{Sports Sciences}}},
  author = {Mesquida, Cristian and Murphy, Jennifer and Lakens, Dani{\"e}l and Warne, Joe},
  year = 2023,
  month = sep,
  journal = {Journal of Sports Sciences},
  volume = {41},
  number = {16},
  pages = {1507--1517},
  issn = {1466-447X},
  doi = {10.1080/02640414.2023.2269357},
  abstract = {Two factors that decrease the replicability of studies in the scientific literature are publication bias and studies with underpowered desgins. One way to ensure that studies have adequate statistical power to detect the effect size of interest is by conducting a-priori power analyses. Yet, a previous editorial published in the Journal of Sports Sciences reported a median sample size of 19 and the scarce usage of a-priori power analyses. We meta-analysed 89 studies from the same journal to assess the presence and extent of publication bias, as well as the average statistical power, by conducting a z-curve analysis. In a larger sample of 174 studies, we also examined a) the usage, reporting practices and reproducibility of a-priori power analyses; and b) the prevalence of reporting practices of t-statistic or F-ratio, degrees of freedom, exact p-values, effect sizes and confidence intervals. Our results indicate that there was some indication of publication bias and the average observed power was low (53\% for significant and non-significant findings and 61\% for only significant findings). Finally, the usage and reporting practices of a-priori power analyses as well as statistical results including test statistics, effect sizes and confidence intervals were suboptimal.},
  langid = {english},
  pmid = {38018365},
  keywords = {Bias,Humans,publication bias,Publication Bias,Replicability,reporting practices,reproducibility,Reproducibility of Results,Research Design,Sample Size,statistical power},
  file = {/Users/cristian/Zotero/storage/WNA9JV57/Mesquida et al. - 2023 - Publication bias, statistical power and reporting .pdf}
}

@misc{mesquida_zcurve_2025,
  title = {On the Replicability of Sports and Exercise Science Research: Assessing the Prevalence of Publication Bias and Studies with Underpowered Designs by a z-Curve Analysis},
  shorttitle = {On the Replicability of Sports and Exercise Science Research},
  author = {Mesquida, Cristian and Murphy, Jennifer and Warne, Joe and Lakens, Dani{\"e}l},
  year = 2025,
  month = apr,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.534},
  urldate = {2025-04-04},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {publication bias,replicability,statistical power},
  file = {/Users/cristian/Zotero/storage/TWSZ9W5Z/Mesquida et al. - 2025 - On the replicability of sports and exercise scienc.pdf}
}

@article{micklewright2017,
  title = {Development and {{Validity}} of the {{Rating-of-Fatigue Scale}}},
  author = {Micklewright, D. and St Clair Gibson, A. and Gladwell, V. and Al Salman, A.},
  year = 2017,
  month = nov,
  journal = {Sports Medicine},
  volume = {47},
  number = {11},
  pages = {2375--2393},
  issn = {1179-2035},
  doi = {10.1007/s40279-017-0711-5},
  urldate = {2025-03-21},
  abstract = {The purpose of these experiments was to develop a rating-of-fatigue (ROF) scale capable of tracking the intensity of perceived fatigue in a variety of contexts.},
  langid = {english},
  keywords = {Electronic Supplementary Material Appendix,Likert Score,Respiratory Exchange Ratio,Ventilation Rate,Volitional Exhaustion},
  file = {/Users/cristian/Zotero/storage/Y5WQ7UDW/Micklewright et al. - 2017 - Development and Validity of the Rating-of-Fatigue .pdf}
}

@article{molloy2022,
  title = {Multiplicity Adjustments in Parallel-Group Multi-Arm Trials Sharing a Control Group: {{Clear}} Guidance Is Needed},
  shorttitle = {Multiplicity Adjustments in Parallel-Group Multi-Arm Trials Sharing a Control Group},
  author = {Molloy, S{\'i}le F. and White, Ian R. and Nunn, Andrew J. and Hayes, Richard and Wang, Duolao and Harrison, Thomas S.},
  year = 2022,
  month = feb,
  journal = {Contemporary Clinical Trials},
  volume = {113},
  pages = {106656},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2021.106656},
  urldate = {2025-04-12},
  abstract = {Multi-arm, parallel-group clinical trials are an efficient way of testing several new treatments, treatment regimens or doses. However, guidance on the requirement for statistical adjustment to control for multiple comparisons (type I error) using a shared control group is unclear. We argue, based on current evidence, that adjustment is not always necessary in such situations. We propose that adjustment should not be a requirement in multi-arm, parallel-group trials testing distinct treatments and sharing a control group, and we call for clearer guidance from stakeholders, such as regulators and scientific journals, on the appropriate settings for adjustment of multiplicity.},
  keywords = {{Multi-arm, parallel-group clinical trials},Family-wise error rate (FWER),Multiplicity,Type 1 error},
  file = {/Users/cristian/Zotero/storage/SZYSAQEI/Molloy et al. - 2022 - Multiplicity adjustments in parallel-group multi-a.pdf;/Users/cristian/Zotero/storage/M5QLIHTD/S155171442100392X.html}
}

@article{murphy_nonsignificance_2025,
  title = {Nonsignificance Misinterpreted as an Effect's Absence in Psychology: Prevalence and Temporal Analyses},
  shorttitle = {Nonsignificance Misinterpreted as an Effect's Absence in Psychology},
  author = {Murphy, Stephen Lee and Merz, Raphael and Reimann, Linda-Elisabeth and Fern{\'a}ndez, Aurelio},
  year = 2025,
  month = mar,
  journal = {Royal Society Open Science},
  volume = {12},
  number = {3},
  pages = {242167},
  issn = {2054-5703},
  doi = {10.1098/rsos.242167},
  urldate = {2025-04-22},
  abstract = {Nonsignificant findings in psychological research are frequently misinterpreted as reflecting the effect's absence. However, this issue's exact prevalence remains unclear, as does whether this issue is getting better or worse. In this pre-registered study, we sought to answer these questions by examining the discussion sections of 599 articles published across 10 psychology journals and three time points (2009, 2015 and 2021), and coding whether a nonsignificant finding was interpreted in such a way as to suggest the effect does not exist. Our models indicate that between 76\% and 85\% of psychology articles published between 2009 and 2021 that discussed a nonsignificant finding misinterpreted nonsignificance as reflecting no effect. It is likely between 54\% and 62\% of articles over this time period claimed explicitly that this meant no effect on the population of interest. Our findings also indicate that only between 4\% and 8\% of articles explicitly discussed the possibility that the nonsignificant effect may exist but could not be found. Differences in prevalence rates over time were nonsignificant. Collectively, our findings indicate this interpretative error is a major problem in psychology. We call on stakeholders with an interest in improving psychological science to prioritize tackling it.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/7PEE2W3L/Murphy et al. - 2025 - Nonsignificance misinterpreted as an effects abse.pdf}
}

@article{murphy_replicability_2025,
  title = {Estimating the {{Replicability}} of {{Sports}} and {{Exercise Science Research}}},
  author = {Murphy, Jennifer and Caldwell, Aaron R. and Mesquida, Cristian and Ladell, Aera J. M. and {Encarnaci{\'o}n-Mart{\'i}nez}, Alberto and Tual, Alexandre and Denys, Andrew and Cameron, Bailey and Van Hooren, Bas and Parr, Ben and DeLucia, Bianca and Mason, Billy R. J. and Clark, Brad and Egan, Brendan and Brown, Calum and Ade, Carl and Sforza, Chiarella and Taber, Christopher B. and Kirk, Christopher and McCrum, Christopher and Tighe, Cian Okeeffe and Byrne, Ciara and Brunetti, Claudia and Forestier, Cyril and Martin, Dan and Taylor, Danny and Diggin, David and Gallagher, Dearbhla and King, Deborah L. and Rogers, Elizabeth and Bennett, Eric C. and Lopatofsky, Eric T. and Dunn, Gemma and Gauchar, G{\'e}rome C. and Mornieux, Guillaume and {Catal{\'a}-Vilaplana}, Ignacio and Caetan, Ines and {Aparicio-Aparicio}, Inmaculada and Barnes, Jack and Blaisdell, Jake and Steele, James and Fletcher, Jared R. and Hutchinson, Jasmin and Au, Jason and Oliemans, Jason P. and Bakhshinejad, Javad and Barrios, Joaquin and Quesada, Jose Ignacio Priego and Rager, Joseph and Capon, Julia B. and Walton, Julie S. J. and Stevens, Kailey and Heinrich, Katie and Wu, Kelly and Meijer, Kenneth and Richards, Laura and Jutlah, Lauren and Tong, Le and Bridgeman, Lee and Banet, Leo and Mbiyu, Leonard and Sefton, Lucy and {de Chanaleilles}, Margaret and Charisi, Maria and Beerse, Matthew and Major, Matthew J. and Caon, Maya and Bargh, Mel and Rowley, Michael and Moran, Miguel Vaca and Croker, Nicholas and Hanen, Nicolas C. and Montague, Nicole and Brick, Noel E. and Runswick, Oliver R. and Willems, Paul and {P{\'e}rez-Soriano}, Pedro and Blake, Rebecca and Jones, Rebecca and Quinn, Rebecca Louise and {Sanchis-Sanchis}, Roberto and Rabello, Rodrigo and Bolger, Roisin and Shohat, Roy and Cotton, Sadie and Chua, Samantha and Norwood, Samuel and Vimeau, Samuel and Dias, Sandro and Pedersen, Sissel and Skaper, Spencer S. and Coyle, Taylor and Desai, Terun and Gee, Thomas I. and Edwards, Tobias and Pohl, Torsten and Yingling, Vanessa and Ribeiro, Vinicius and Duchene, Youri and Papadakis, Zacharias and Warne, Joe P.},
  year = 2025,
  journal = {Sports Medicine},
  issn = {1179-2035},
  doi = {10.1007/s40279-025-02201-w},
  urldate = {2025-06-17},
  abstract = {The replicability of sports and exercise research has not been assessed previously despite concerns about scientific practices within the field.},
  langid = {english},
  keywords = {Communication and replication,Experimental Psychology,Psychometrics,Sport Psychology,Sport Science,Sport Theory},
  file = {/Users/cristian/Zotero/storage/JWDTUMR6/Murphy et al. - 2025 - Estimating the Replicability of Sports and Exercis.pdf}
}

@article{murphy1999,
  ids = {murphy_minimum-effect-test_1999},
  title = {Testing the Hypothesis That Treatments Have Negligible Effects: {{Minimum-effect}} Tests in the General Linear Model},
  shorttitle = {Testing the Hypothesis That Treatments Have Negligible Effects},
  author = {Murphy, Kevin R. and Myors, Brett},
  year = 1999,
  journal = {Journal of Applied Psychology},
  volume = {84},
  number = {2},
  pages = {234--248},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1854},
  doi = {10.1037/0021-9010.84.2.234},
  abstract = {Researchers are often interested in testing the hypothesis that the effects of treatments, interventions, and so on are negligibly small rather than testing the hypothesis that treatments have no effect whatsoever. A number of procedures for conducting such tests have been suggested but have yet to be widely adopted. In this article, simple methods of testing such minimum-effect hypotheses are illustrated in a variety of applications of the general linear model. Tables and computational routines that can be used in conjunction with the familiar F test to evaluate the hypothesis that the effects of treatments or interventions exceed some minimum level are also provided. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Hypothesis Testing,Null Hypothesis Testing,Statistical Tests,Treatment,Treatment Effectiveness Evaluation},
  file = {/Users/cristian/Zotero/storage/MVVB26AS/1999-13895-007.html}
}

@article{nakagawa2004,
  title = {A Farewell to {{Bonferroni}}: The Problems of Low Statistical Power and Publication Bias},
  shorttitle = {A Farewell to {{Bonferroni}}},
  author = {Nakagawa, Shinichi},
  year = 2004,
  month = nov,
  journal = {Behavioral Ecology},
  volume = {15},
  number = {6},
  pages = {1044--1045},
  issn = {1045-2249},
  doi = {10.1093/beheco/arh107},
  urldate = {2024-08-23},
  file = {/Users/cristian/Zotero/storage/VE2VSNPJ/Nakagawa - 2004 - A farewell to Bonferroni the problems of low stat.pdf;/Users/cristian/Zotero/storage/CR6ES65B/206216.html}
}

@article{nickerson2000,
  title = {Null Hypothesis Significance Testing: {{A}} Review of an Old and Continuing Controversy},
  shorttitle = {Null Hypothesis Significance Testing},
  author = {Nickerson, Raymond S.},
  year = 2000,
  journal = {Psychological Methods},
  volume = {5},
  pages = {241--301},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.5.2.241},
  abstract = {Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing,Statistical Analysis},
  file = {/Users/cristian/Zotero/storage/P28F45R9/2000-07827-007.html}
}

@article{parker2020,
  title = {Non-Adjustment for Multiple Testing in Multi-Arm Trials of Distinct Treatments: {{Rationale}} and Justification},
  shorttitle = {Non-Adjustment for Multiple Testing in Multi-Arm Trials of Distinct Treatments},
  author = {Parker, Richard A and Weir, Christopher J},
  year = 2020,
  month = oct,
  journal = {Clinical Trials (London, England)},
  volume = {17},
  number = {5},
  pages = {562--566},
  issn = {1740-7745},
  doi = {10.1177/1740774520941419},
  urldate = {2024-08-23},
  abstract = {There is currently a lack of consensus and uncertainty about whether one should adjust for multiple testing in multi-arm trials of distinct treatments. A detailed rationale is presented to justify non-adjustment in this situation. We argue that non-adjustment should be the default starting position in simple multi-arm trials of distinct treatments.},
  pmcid = {PMC7534018},
  pmid = {32666813},
  file = {/Users/cristian/Zotero/storage/WPZYL766/Parker and Weir - 2020 - Non-adjustment for multiple testing in multi-arm t.pdf}
}

@article{perugini2025,
  title = {The {{Benefits}} of {{Reporting Critical-Effect-Size Values}}},
  author = {Perugini, Ambra and Gambarota, Filippo and Toffalini, Enrico and Lakens, Dani{\"e}l and Pastore, Massimiliano and Finos, Livio and Alto{\`e}, Gianmarco},
  year = 2025,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {8},
  number = {2},
  pages = {25152459251335298},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459251335298},
  urldate = {2025-06-07},
  abstract = {Critical-effect-size values represent the smallest detectable effect that can reach statistical significance given a specific sample size, alpha level, and test statistic. It can be useful to calculate the critical effect size when designing a study and evaluate whether such effects are plausible. Reporting critical-effect-size values may be useful when the sample size has not been planned a priori, there is uncertainty about the expected sample size that can be collected, or researchers plan to analyze the data with a statistical hypothesis test. To assist researchers in calculating critical-effect-size values, we developed an R package that allows researchers to report critical-effect-size values for group comparisons, correlations, linear regressions, and meta-analyses. Reflecting on critical-effect-size values could benefit researchers during the planning phase of the study by helping them to understand the limitations of their research design. Critical-effect-size values are also useful when evaluating studies performed by other researchers when a priori power analyses were not performed, especially when nonsignificant results are observed.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/3FLIYQSY/Perugini et al. - 2025 - The Benefits of Reporting Critical-Effect-Size Val.pdf}
}

@article{rubin2021,
  title = {When to Adjust Alpha during Multiple Testing: A Consideration of Disjunction, Conjunction, and Individual Testing},
  shorttitle = {When to Adjust Alpha during Multiple Testing},
  author = {Rubin, Mark},
  year = 2021,
  journal = {Synthese},
  volume = {199},
  number = {3},
  pages = {10969--11000},
  publisher = {Springer},
  file = {/Users/cristian/Zotero/storage/ETHMXVFW/Rubin - 2021 - When to adjust alpha during multiple testing a co.pdf;/Users/cristian/Zotero/storage/RJY5J9L8/s11229-021-03276-4.html}
}

@article{schardt2007,
  title = {Utilization of the {{PICO}} Framework to Improve Searching {{PubMed}} for Clinical Questions},
  author = {Schardt, Connie and Adams, Martha B. and Owens, Thomas and Keitz, Sheri and Fontelo, Paul},
  year = 2007,
  month = jun,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {7},
  number = {1},
  pages = {16},
  issn = {1472-6947},
  doi = {10.1186/1472-6947-7-16},
  urldate = {2025-03-24},
  abstract = {Supporting 21st century health care and the practice of evidence-based medicine (EBM) requires ubiquitous access to clinical information and to knowledge-based resources to answer clinical questions. Many questions go unanswered, however, due to lack of skills in formulating questions, crafting effective search strategies, and accessing databases to identify best levels of evidence.},
  langid = {english},
  keywords = {Clinical Question,Percutaneous Endoscopic Gastrostomy,Percutaneous Endoscopic Gastrostomy Tube,Publication Type,Relevant Citation},
  file = {/Users/cristian/Zotero/storage/B4JZEEUA/Schardt et al. - 2007 - Utilization of the PICO framework to improve searc.pdf}
}

@article{scheel_notevenwrong2022,
  title = {Why Most Psychological Research Findings Are Not Even Wrong},
  author = {Scheel, Anne M.},
  year = 2022,
  journal = {Infant and Child Development},
  volume = {31},
  number = {1},
  pages = {e2295},
  issn = {1522-7219},
  doi = {10.1002/icd.2295},
  urldate = {2022-09-06},
  abstract = {Psychology's replication crisis is typically conceptualized as the insight that the published literature contains a worrying amount of unreplicable, false-positive findings. At the same time, meta-scientific attempts to assess the crisis in more detail have reported substantial difficulties in identifying unambiguous definitions of the scientific claims in published articles and determining how they are connected to the presented evidence. I argue that most claims in the literature are so critically underspecified that attempts to empirically evaluate them are doomed to failure---they are not even wrong. Meta-scientists should beware of the flawed assumption that the psychological literature is a collection of well-defined claims. To move beyond the crisis, psychologists must reconsider and rebuild the conceptual basis of their hypotheses before trying to test them.},
  langid = {english},
  keywords = {falsification,hypothesis testing,replication crisis,reproducibility,scientific inference},
  file = {/Users/cristian/Zotero/storage/RBRTCHK2/Scheel - 2022 - Why most psychological research findings are not e.pdf;/Users/cristian/Zotero/storage/R3T3B9KV/icd.html}
}

@article{scheel_why_hypothesis,
  title = {Why {{Hypothesis Testers Should Spend Less Time Testing Hypotheses}}},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Dani{\"e}l},
  year = 2020,
  month = dec,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  pages = {1745691620966795},
  issn = {1745-6924},
  doi = {10.1177/1745691620966795},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound "derivation chain" between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology's reform movement and help us to develop strong, testable theories, as Paul Meehl urged.},
  langid = {english},
  pmid = {33326363},
  keywords = {exploratory research,hypothesis testing,replication crisis},
  file = {/Users/cristian/Zotero/storage/56TKYQ2L/Scheel et al. - 2020 - Why Hypothesis Testers Should Spend Less Time Test.pdf}
}

@book{schmidt2015,
  title = {Methods of {{Meta-Analysis}}: {{Correcting Error}} and {{Bias}} in {{Research Findings}}},
  shorttitle = {Methods of {{Meta-Analysis}}},
  author = {Schmidt, Frank L. and Hunter, John E.},
  year = 2015,
  publisher = {SAGE Publications, Ltd},
  doi = {10.4135/9781483398105},
  urldate = {2025-10-28},
  abstract = {{$<$}p{$>$}Designed to provide researchers clear and informative insight into techniques of meta-analysis, the Third Edition of Methods of Meta-Analysis: Correcting Err},
  isbn = {978-1-4833-9810-5},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/XFJ2Y4WV/Schmidt and Hunter - 2015 - Methods of Meta-Analysis Correcting Error and Bias in Research Findings.pdf}
}

@book{senn2008,
  title = {Statistical {{Issues}} in {{Drug Development}}},
  author = {Senn, Stephen J.},
  year = 2008,
  edition = {Second},
  publisher = {John Wiley \& Sons, Ltd},
  urldate = {2025-08-22}
}

@article{stefan_2023,
  ids = {stefanBigLittleLies2022},
  title = {Big Little Lies: A Compendium and Simulation of p-Hacking Strategies},
  shorttitle = {Big Little Lies},
  author = {Stefan, A. M. and Sch{\"o}nbrodt, Felix D.},
  year = 2023,
  month = feb,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {220346},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220346},
  urldate = {2023-02-13},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12 p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  keywords = {error rates,Error Rates,False Positive Results,false-positive rate,Meta-science,p-curve,p-Curve,Quantitative Methods,questionable research practices,Questionable Research Practices,Shiny app,Shiny App,significance,Significance,simulation,Simulation,Social and Behavioral Sciences,Statistical Methods},
  file = {/Users/cristian/Zotero/storage/P5IJHMR6/Stefan and Schnbrodt - 2022 - Big Little Lies A Compendium and Simulation of p-.pdf;/Users/cristian/Zotero/storage/X7DJUPS5/Stefan and Schnbrodt - 2023 - Big little lies a compendium and simulation of p-.pdf}
}

@article{stewart2000,
  title = {Detecting Dose Response with Contrasts},
  author = {Stewart, W. H. and Ruberg, S. J.},
  year = 2000,
  month = apr,
  journal = {Statistics in Medicine},
  volume = {19},
  number = {7},
  pages = {913--921},
  issn = {0277-6715},
  doi = {10.1002/(sici)1097-0258(20000415)19:7<913::aid-sim397>3.0.co;2-2},
  abstract = {Analyses of dose response studies should separate the question of the existence of a dose response relationship from questions of functional form and finding the optimal dose. A well-chosen contrast among the estimated effects of the studied doses can make a powerful test for detecting the existence of a dose response relationship. A contrast-based test attains its greatest power when the pattern of the coefficients has the same shape as the true dose response relationship. However, it loses power when the contrast shape and the true dose response shape are not similar. Thus, a primary test based on a single contrast is often risky. Two (or more) appropriately chosen contrasts can assure sufficient power to justify the cost of a multiplicity adjustment. An example shows the success of a two-contrast procedure in detecting dose response, which had frustrated several standard procedures.},
  langid = {english},
  pmid = {10750059},
  keywords = {{Data Interpretation, Statistical},{Dose-Response Relationship, Drug},{Drug Evaluation, Preclinical},Antiemetics,Clinical Trials as Topic,Humans,Nausea,Randomized Controlled Trials as Topic,Research Design,Sample Size,Vomiting}
}

@article{strategic_ambiguity,
  title = {Strategic Ambiguity in the Social Sciences},
  author = {Frankenhuis, Willem and Panchanathan, Karthik and Smaldino, Paul E.},
  year = 2022,
  month = jul,
  publisher = {MetaArXiv},
  doi = {10.31222/osf.io/kep5b},
  urldate = {2022-10-08},
  abstract = {In the wake of the replication crisis, there have been calls to increase the clarity and precision of theory in the social sciences. Here, we argue that the effects of these calls may be limited due to systematic and structural factors, and focus our attention on incentives favoring ambiguous theory. Intentionally or not, scientists can exploit theoretical ambiguities to make support for a claim appear stronger than is warranted. Practices include `theory stretching', interpreting an ambiguous claim more expansively to absorb data outside of the scope of the original claim, and `post-hoc precision', interpreting an ambiguous claim more narrowly so it appears more precisely aligned with the data. These practices lead to the overestimation of evidence for the original claim and create the appearance of consistent support and progressive research programs, which may in turn be rewarded by journals, funding agencies, and hiring committees. Selection for ambiguous research can occur outside of scientists' awareness and even when scientists act in good faith. We supplement our verbal argument with a simple mathematical model. Although ambiguity might be inevitable or even useful in the early stages of theory construction, scientists should aim for increased clarity as knowledge advances. Science benefits from transparently communicating about known ambiguities. To attain transparency about ambiguity, we provide a set of recommendations for authors, reviewers, and journals. We conclude with suggestions for research on how scientists use strategic ambiguity to advance their careers and on the ways in which norms, incentives, and practices favor strategic ambiguity.},
  langid = {american},
  keywords = {formal modeling,incentive structures,Other Social and Behavioral Sciences,post-hoc precision,Psychology,Social and Behavioral Sciences,strategic ambiguity,theory development,theory stretching}
}

@article{tunc2023,
  ids = {tuncEpistemicPragmaticFunction2021},
  title = {The Epistemic and Pragmatic Function of Dichotomous Claims Based on Statistical Hypothesis Tests},
  author = {Tun{\c c}, Duygu and Tun{\c c}, Mehmet Necip and Lakens, Dani{\"e}l},
  year = 2023,
  month = jun,
  journal = {Theory \& Psychology},
  volume = {33},
  number = {3},
  pages = {403--423},
  publisher = {SAGE Publications Ltd},
  issn = {0959-3543},
  doi = {10.1177/09593543231160112},
  urldate = {2023-08-09},
  abstract = {Researchers commonly make dichotomous claims based on continuous test statistics. Many have branded the practice as a misuse of statistics and criticize scientists for the widespread application of hypothesis tests to tentatively reject a hypothesis (or not) depending on whether a p-value is below or above an alpha level. Although dichotomous claims are rarely explicitly defended, we argue they play an important epistemological and pragmatic role in science. The epistemological function of dichotomous claims consists in transforming data into quasibasic statements, which are tentatively accepted singular facts that can corroborate or falsify theoretical claims. This transformation requires a prespecified methodological decision procedure such as Neyman-Pearson hypothesis tests. From the perspective of methodological falsificationism these decision procedures are necessary, as probabilistic statements (e.g., continuous test statistics) cannot function as falsifiers of substantive hypotheses. The pragmatic function of dichotomous claims is to facilitate scrutiny and criticism among peers by generating contestable claims, a process referred to by Popper as ``conjectures and refutations.'' We speculate about how the surprisingly widespread use of a 5\% alpha level might have facilitated this pragmatic function. Abandoning dichotomous claims, for example because researchers commonly misuse p-values, would sacrifice their crucial epistemic and pragmatic functions.},
  langid = {english},
  keywords = {basic statements,dichotomous claims,methodological falsificationism,Quantitative Methods,Social and Behavioral Sciences,statisitical hypothesis testing,Theory and Philosophy of Science,theory testing},
  file = {/Users/cristian/Zotero/storage/8HGI3MCE/Tun et al. - 2021 - The Epistemic and Pragmatic Function of Dichotomou.pdf;/Users/cristian/Zotero/storage/QI7Z5NHL/Uygun Tun et al. - 2023 - The epistemic and pragmatic function of dichotomou.pdf}
}

@article{sainani_collaboration,
	title = {Call to increase statistical collaboration in sports science, sport and exercise medicine and sports physiotherapy},
	author = {Sainani, Kristin L. and Borg, David N. and Caldwell, Aaron R. and Butson, Michael L. and Tenan, Matthew S. and Vickers, Andrew J. and Vigotsky, Andrew D. and Warmenhoven, John and Nguyen, Robert and Lohse, Keith R. and Knight, Emma J. and Bargary, Norma},
	year = {2021},
	month = {01},
	date = {2021-01-01},
	journal = {British Journal of Sports Medicine},
	pages = {118--122},
	volume = {55},
	number = {2},
	doi = {10.1136/bjsports-2020-102607},
	url = {https://bjsm.bmj.com/content/55/2/118},
	note = {Citation Key: sainani{\_}collaboration},
	langid = {en}
}

@article{vickers2001,
	title = {Analysing controlled trials with baseline and follow up measurements},
	author = {Vickers, Andrew J. and Altman, Douglas G.},
	year = {2001},
	month = {11},
	date = {2001-11-10},
	journal = {BMJ},
	pages = {1123--1124},
	volume = {323},
	number = {7321},
	doi = {10.1136/bmj.323.7321.1123},
	url = {https://www.bmj.com/content/323/7321/1123},
	note = {Publisher: British Medical Journal Publishing Group
Section: Education and debate
PMID: 11701584},
	langid = {en}
}

@article{oconnell2017,
	title = {Methods for Analysis of Pre-Post Data in Clinical Research: A Comparison of Five Common Methods},
	author = {{O'Connell}, Nathaniel S. and Dai, Lin and Jiang, Yunyun and Speiser, Jaime L. and Ward, Ralph and Wei, Wei and Carroll, Rachel and Gebregziabher, Mulugeta},
	year = {2017},
	month = {02},
	date = {2017-02-24},
	journal = {Journal of biometrics & biostatistics},
	pages = {1--8},
	volume = {8},
	number = {1},
	doi = {10.4172/2155-6180.1000334},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC6290914/},
	note = {PMID: 30555734
PMCID: PMC6290914}
}

@article{keene2023,
	title = {Why estimands are needed to define treatment effects in clinical trials},
	author = {Keene, Oliver N. and Lynggaard, Helle and Englert, Stefan and Lanius, Vivian and Wright, David},
	year = {2023},
	month = {07},
	date = {2023-07-27},
	journal = {BMC Medicine},
	pages = {276},
	volume = {21},
	number = {1},
	doi = {10.1186/s12916-023-02969-6},
	url = {https://doi.org/10.1186/s12916-023-02969-6}
}

@article{lakens_deviations_2024,
	title = {When and How to Deviate From a Preregistration},
	author = {Lakens, {Danil}},
	editor = {Ravenzwaaij, Don van},
	year = {2024},
	month = {05},
	date = {2024-05-14},
	journal = {Collabra: Psychology},
	pages = {117094},
	volume = {10},
	number = {1},
	doi = {10.1525/collabra.117094},
	url = {https://doi.org/10.1525/collabra.117094},
	note = {Citation Key: lakens{\_}deviations{\_}2024
tex.ids= lakens{\_}when{\_}2024}
}

@article{ICH9,
	title = {ICH E9(R1) Addendum on Estimands and Sensitivity Analysis in Clinical Trials (2019)},
	author = {International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use., },
	year = {2019},
	date = {2019},
	url = {https://database.ich.org/sites/default/files/E9-R1_Step4_Guideline_2019_1203.pdf},
	note = {Citation Key: ICH9}
}

@article{cook_2014,
	title = {Assessing methods to specify the target difference for a randomised controlled trial: DELTA (Difference ELicitation in TriAls) review},
	author = {Cook, Jonathan A. and Hislop, Jennifer and Adewuyi, Temitope E. and Harrild, Kirsten and Altman, Douglas G. and Ramsay, Craig R. and Fraser, Cynthia and Buckley, Brian and Fayers, Peter and Harvey, Ian and Briggs, Andrew H. and Norrie, John D. and Fergusson, Dean and Ford, Ian and Vale, Luke D.},
	year = {2014},
	date = {2014},
	journal = {Health Technology Assessment (Winchester, England)},
	volume = {18},
	number = {28},
	doi = {10.3310/hta18280},
	note = {PMID: 24806703
PMCID: PMC4781097
Citation Key: cook{\_}2014},
	langid = {eng}
}

@article{keefe_2013,
	title = {Defining a Clinically Meaningful Effect for the Design and Interpretation of Randomized Controlled Trials},
	author = {Keefe, Richard S. E. and Kraemer, Helena C. and Epstein, Robert S. and Frank, Ellen and Haynes, Ginger and Laughren, Thomas P. and Mcnulty, James and Reed, Shelby D. and Sanchez, Juan and Leon, Andrew C.},
	year = {2013},
	date = {2013},
	journal = {Innovations in Clinical Neuroscience},
	pages = {4S--19S},
	volume = {10},
	number = {5-6 Suppl A},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3719483/},
	note = {PMID: 23882433
PMCID: PMC3719483
Citation Key: keefe{\_}2013}
}
